---
title: "Lab 7"
author: "Cameron Adams"
date: "Math 241, Week 9"
output:
  pdf_document
urlcolor: blue
---

```{r setup, include=FALSE}
# Do not modify this chunk.
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)


```

```{r}
install.packages("wordcloud")



```

```{r}
# Put all necessary libraries here
library(tidyverse)
library(tidytext)
library(gutenbergr)
library(dplyr)
library(tidyr)
library(tidytext)
library(ggplot2)
library(wordcloud)

# Ensure the textdata package is installed
if (!requireNamespace("textdata", quietly = TRUE)) {
  install.packages("textdata")
}
# Load the textdata package
library(textdata)

# Before knitting your document one last time, you will have to download the AFINN lexicon explicitly
lexicon_afinn()
lexicon_nrc()
```



## Due: Friday, March 29th at 5:30pm

## Goals of this lab

1. Practice matching patterns with regular expressions.
1. Practice manipulating strings with `stringr`.
1. Practice tokenizing text with `tidytext`.
1. Practice looking at word frequencies.
1. Practice conducting sentiment analysis.


### Problem 1: What's in a Name?  (You'd Be Surprised!)
  
1. Load the `babynames` dataset, which contains yearly information on the frequency of baby names by sex and is provided by the US Social Security Administration.  It includes all names with at least 5 uses per year per sex. In this problem, we are going to practice pattern matching!

```{r}
library(babynames)
data("babynames")
#?babynames
```

a. For 2000, find the ten most popular female baby names that start with the letter Z.

```{r, eval = FALSE}
female_names_2000_Z <- babynames %>%
  filter(year == 2000, sex == "F", startsWith(name, "Z"))

top_female_names_2000_Z <- female_names_2000_Z %>%
  arrange(desc(n)) %>%
  top_n(10)

top_female_names_2000_Z
```




b. For 2000, find the ten most popular female baby names that contain the letter z.  

```{r}
female_names_2000_C_Z <- babynames %>%
  filter(year == 2000, sex == "F", str_detect(name, "z")|startsWith(name, "Z")|str_sub(name, -1) == "z")

top_female_names_2000_C_Z <- female_names_2000_C_Z %>%
  arrange(desc(n)) %>%
  top_n(10)

top_female_names_2000_C_Z
```



c. For 2000, find the ten most popular female baby names that end in the letter z. 

```{r}
female_names_2000_end_Z <- babynames %>%
  filter(year == 2000, sex == "F", str_sub(name, -1) == "z")

top_female_names_2000_end_Z <- female_names_2000_end_Z %>%
  arrange(desc(n)) %>%
  top_n(10)

top_female_names_2000_end_Z
```


d. Between your three tables in 1.a - 1.c, do any of the names show up on more than one list?  If so, which ones? (Yes, I know you could do this visually but use some joins!)


```{r}
# Perform inner joins between the tables
joined_tables1 <- inner_join(top_female_names_2000_Z, top_female_names_2000_C_Z, by = "name") 

joined_tables2 <- inner_join(top_female_names_2000_Z, top_female_names_2000_end_Z, by = "name")

joined_tables3 <- inner_join(top_female_names_2000_end_Z, top_female_names_2000_C_Z, by = "name") 
# Select the common names
common_names1 <- joined_tables1$name
common_names2 <- joined_tables2$name
common_names3 <- joined_tables3$name


# Print the common names
common_names1
common_names2
common_names3

```

From here we can see Zoe is the only one that is shared between lists.


e.  Verify that none of the baby names contain a numeric (0-9) in them.

```{r}
names_with_numeric <- babynames %>%
  filter(str_detect(name, "[0-9]")) %>%
  pull(name)

if (length(names_with_numeric) > 0) {
  cat("Baby names containing numeric digits:\n")
  cat(names_with_numeric, sep = "\n")
} else {
  print("No baby names contain numeric digits.")
}

```



f. While none of the names contain 0-9, that doesn't mean they don't contain "one", "two", ..., or "nine".  Create a table that provides the number of times a baby's name contained the word "zero", the word "one", ... the word "nine". 

Notes: 

* I recommend first converting all the names to lower case.
* If none of the baby's names contain the written number, there you can leave the number out of the table.
* Use `str_extract()`, not `str_extract_all()`. (We will ignore names where more than one of the words exists.)

*Hint*: You will have two steps that require pattern matching:
    1. Subset your table to only include the rows with the desired words.
    2. Add a column that contains the desired word.  


```{r}

#lowercase
babynames_lower <- babynames %>%
  mutate(name = tolower(name))

numbers_words <- c("zero", "one", "two", "three", "four", "five", "six", "seven", "eight", "nine")

extracted_numbers <- sapply(numbers_words, function(word) {
  count <- babynames_lower %>%
    filter(str_detect(name, paste0("\\b", word, "\\b"))) %>%
    summarise(count = n())
  count <- ifelse(is.na(count$count), 0, count$count)
  return(count)
})

numbers_table <- data.frame(Number = numbers_words, Count = extracted_numbers)

print(numbers_table)

```

g. Which written number or numbers don't show up in any of the baby names?

the numbers zero and seven showed up, with 7 showing up 49 times and zero showing up once



h. Create a table that contains the names and their frequencies for the two least common written numbers.

```{r}


# Convert names to lowercase
babynames_lower <- babynames %>%
  mutate(name = tolower(name))

# Filter the dataset for the least common written numbers: zero and seven
least_common_numbers <- c("zero", "seven")

# Initialize an empty list to store results
least_common_names <- list()

# Loop through each least common number
for (number in least_common_numbers) {
  # Filter the dataset for names containing the current number
  filtered_data <- babynames_lower %>%
    filter(str_detect(name, paste0("\\b", number, "\\b")))
  
  # Summarize to get the frequency count for each name
  summarized_data <- filtered_data %>%
    group_by(name) %>%
    summarize(frequency = n()) %>%
    arrange(frequency)
  
  # Select the top two rows
  top_names <- head(summarized_data, 2)
  
  # Add the result to the list
  least_common_names[[number]] <- top_names
}

# Combine the results into a single table
least_common_table <- do.call(rbind, least_common_names)

# Print the table
print(least_common_table)

```



i. List out the names that contain no vowels (consider "y" to be a vowel).  

```{r}
library(stringr)

# Function to remove vowels from names (considering 'y' as a vowel)
remove_vowels <- function(name) {
  str_remove_all(tolower(name), "[aeiouy]")
}

# Apply the function to remove vowels from all names
babynames_processed <- babynames %>%
  mutate(processed_name = remove_vowels(name))

# Group by processed names and count occurrences
names_grouped_count <- babynames_processed %>%
  group_by(processed_name) %>%
  summarise(count = n()) %>%
  filter(processed_name != "")  # Filter out empty processed names

# Print the grouped names and counts
print(names_grouped_count)


```


### Problem 2: Tidying the "Call of the Wild"

Did you read "Call of the Wild" by Jack London?  If not, [read the first paragraph of its wiki page](https://en.wikipedia.org/wiki/The_Call_of_the_Wild) for a quick summary and then let's do some text analysis on this classic!  The following code will pull the book into R using the `gutenbergr` package.  

```{r}
library(gutenbergr)
wild <- gutenberg_download(215)
```

a.  Create a tidy text dataset where you tokenize by words.

```{r}
# Create a tidy text dataset by tokenizing words
wild_tokens <- wild %>%
  unnest_tokens(word, text)

# Remove stop words
stop_words <- get_stopwords("en")
wild_tokens <- wild_tokens %>%
  anti_join(stop_words)
```


b. Find the frequency of the 20 most common words.  First, remove stop words.

```{r}
# Find the frequency of the 20 most common words
top_words <- wild_tokens %>%
  count(word, sort = TRUE) %>%
  slice(1:20)

head(top_words)
```

c. Create a bar graph and a word cloud of the frequencies of the 20 most common words.

```{r}


# Create a bar graph of the frequencies of the 20 most common words
ggplot(top_words, aes(x = reorder(word, n), y = n)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(title = "Top 20 Most Common Words in 'Call of the Wild'",
       x = "Word", y = "Frequency") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Create a word cloud of the frequencies of the 20 most common words

library(RColorBrewer)
pal <- brewer.pal(9, "Set1")



wordcloud(words = top_words$word, freq = top_words$n, scale=c(3,0.5), min.freq=1,
          max.words=Inf, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))

```

d. Explore the sentiment of the text using three of the sentiment lexicons in `tidytext`. What does your analysis say about the sentiment of the text?

Notes:

* Make sure to NOT remove stop words this time.  
* `afinn` is a numeric score and should be handled differently than the categorical scores.

```{r}
# Load required libraries
library(tidyr)
library(dplyr)
library(tidytext)

# Tokenize the text without removing stop words
wild_tokens <- wild %>%
  unnest_tokens(word, text)

# Add sentiments using AFINN lexicon
wild_sentiment_afinn <- wild_tokens %>%
  inner_join(get_sentiments("afinn"), by = "word")

# Add sentiments using Bing lexicon
wild_sentiment_bing <- wild_tokens %>%
  inner_join(get_sentiments("bing"), by = "word")

# Add sentiments using NRC lexicon
wild_sentiment_nrc <- wild_tokens %>%
  inner_join(get_sentiments("nrc"), by = "word")

#Convert sentiment to numeric score
wild_sentiment_bing <- wild_sentiment_bing %>%
  mutate(score = ifelse(sentiment == "positive", 1, ifelse(sentiment == "negative", -1, 0)))

# Summarize sentiment scores
summary_bing <- summarise(wild_sentiment_bing, sentiment = sum(score))

# Print the summary result
print("Summary of Bing Sentiment:")
print(summary_bing)



# Print the summary results
print("Summary of AFINN Sentiment:")
print(summary_afinn)
print("Summary of Bing Sentiment:")
print(summary_bing)
print("Summary of NRC Sentiment:")
head(wild_sentiment_nrc)

```





e. If you didn't do so in 2.d, compute the average sentiment score of the text using `afinn`.  Which positive words had the biggest impact? Which negative words had the biggest impact?


```{r}
# Filter out NA values in sentiment scores
wild_sentiment_afinn_filtered <- wild_sentiment_afinn %>%
  filter(!is.na(value))

# Compute average sentiment score using AFINN
avg_sentiment_afinn <- mean(wild_sentiment_afinn_filtered$value)

# Print average sentiment score
print(paste("Average sentiment score (AFINN):", avg_sentiment_afinn))

# Get the words with the highest positive and negative impact
top_positive_words <- head(wild_sentiment_afinn_filtered[order(wild_sentiment_afinn_filtered$value, decreasing = TRUE), ], 5)
top_negative_words <- head(wild_sentiment_afinn_filtered[order(wild_sentiment_afinn_filtered$value), ], 5)

# Print the top positive and negative words
print("Top positive words:")
print(top_positive_words$word)
print("Top negative words:")
print(top_negative_words$word)

```


f. You should have found that "no" was an important negative word in the sentiment score.  To know if that really makes sense, let's turn to the raw lines of text for context.  Pull out all of the lines that have the word "no" in them.  Make sure to not pull out extraneous lines (e.g., a line with the word "now").  

```{r}
# Filter lines containing the word "no"
lines_with_no <- wild %>%
  filter(grepl("\\bno\\b", text, ignore.case = TRUE))


print(lines_with_no$text)

```

g. Draw some conclusions about how "no" is used in the text.

for almost all cases no is used as a negation, not as a negative word. So for example no longer, no slight task, no longer marched, quarrel no more,  no men , no slack, no thousand dollars. These are all used to negate the discription. 


h. We can also look at how the sentiment of the text changes as the text progresses.  Below, I have added two columns to the original dataset. Now I want you to do the following wrangling:

* Tidy the data (but don't drop stop words).
* Add the word sentiments using `bing`.
* Count the frequency of sentiments by index.
* Reshape the data to be wide with the count of the negative sentiments in one column and the positive in another, along with a column for index.
* Compute a sentiment column by subtracting the negative score from the positive.
    

```{r}
wild_time <- wild %>%
  mutate(line = row_number(), index = floor(line/45) + 1) 
```

```{r, eval = FALSE}

#Hint: fill = 0 will insert zero instead of NA
#pivot_XXX(..., values_fill = 0)
```

```{r}
# Load required libraries
library(tidyr)
library(dplyr)
library(tidytext)

# Tidy the data (but don't drop stop words)
wild_tokens <- wild_time %>%
  unnest_tokens(word, text)

# Add word sentiments using Bing lexicon
wild_sentiment_bing <- wild_tokens %>%
  inner_join(get_sentiments("bing"), by = "word")

# Count the frequency of sentiments by index
sentiment_counts <- wild_sentiment_bing %>%
  count(index, sentiment)

# Reshape the data to be wide with the count of the negative sentiments in one column and the positive in another, along with a column for index
sentiment_counts_wide <- sentiment_counts %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0)

# Compute a sentiment column by subtracting the negative score from the positive
sentiment_counts_wide <- sentiment_counts_wide %>%
  mutate(sentiment = positive - negative)

# Print the resulting dataset
print(sentiment_counts_wide)

```



i. Create a plot of the sentiment scores as the text progresses.

```{r}
# Create a plot of the sentiment scores as the text progresses
library(ggplot2)

ggplot(sentiment_counts_wide, aes(x = index, y = sentiment)) +
  geom_line() +
  labs(title = "Sentiment Scores as Text Progresses",
       x = "Index",
       y = "Sentiment Score")

```



j. The choice of 45 lines per chunk was pretty arbitrary.  Try modifying the index value a few times and recreating the plot in i.  Based on your plots, what can you conclude about the sentiment of the novel as it progresses?

```{r}
# Modify the index value and recreate the plot
wild_time_modified <- wild %>%
  mutate(line = row_number(), index = floor(line/30) + 1) 

wild_tokens_modified <- wild_time_modified %>%
  unnest_tokens(word, text)

wild_sentiment_bing_modified <- wild_tokens_modified %>%
  inner_join(get_sentiments("bing"), by = "word")

sentiment_counts_modified <- wild_sentiment_bing_modified %>%
  count(index, sentiment)

sentiment_counts_wide_modified <- sentiment_counts_modified %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0)

sentiment_counts_wide_modified <- sentiment_counts_wide_modified %>%
  mutate(sentiment = positive - negative)

# Create a plot of the sentiment scores with modified index values
ggplot(sentiment_counts_wide_modified, aes(x = index, y = sentiment)) +
  geom_line() +
  labs(title = "Sentiment Scores as Text Progresses (Modified Index)",
       x = "Index",
       y = "Sentiment Score")

```


k. Let's look at the bigrams (2 consecutive words).  Tokenize the text by bigrams.  

```{r}
# Load required libraries
library(dplyr)
library(tidytext)

# Tokenize the text by bigrams
wild_bigrams <- wild %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

# View the first few rows of the bigrams
head(wild_bigrams)

```


l.  Produce a sorted table that counts the frequency of each bigram and notice that stop words are still an issue.

```{r}
# Load required libraries
library(dplyr)
library(tidytext)

# Tokenize the text by bigrams
wild_bigrams <- wild %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

# Remove stop words
stop_words <- get_stopwords("en")
wild_bigrams <- wild_bigrams %>%
  anti_join(stop_words, by = c("bigram" = "word"))

# Count the frequency of each bigram
bigram_counts <- wild_bigrams %>%
  count(bigram, sort = TRUE)

# View the sorted table of bigram frequencies
print("Sorted table of bigram frequencies:")
print(bigram_counts)

```


